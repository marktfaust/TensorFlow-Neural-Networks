{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marktfaust/TensorFlow-Neural-Networks/blob/main/Faust_Mark_assignment_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84GqNz8Ztw4G"
      },
      "source": [
        "# Artificial Intelligence\n",
        "# 464/664\n",
        "# Assignment #7\n",
        "\n",
        "## General Directions for this Assignment\n",
        "\n",
        "00. We're using a Jupyter Notebook environment (tutorial available here: https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html),\n",
        "01. Output format should be exactly as requested (it is your responsibility to make sure notebook looks as expected on Gradescope),\n",
        "02. Check submission deadline on Gradescope,\n",
        "03. Rename the file to Last_First_assignment_7,\n",
        "04. Submit your notebook (as .ipynb, not PDF) using Gradescope, and\n",
        "05. Do not submit any other files.\n",
        "\n",
        "## Before You Submit...\n",
        "\n",
        "1. Re-read the general instructions provided above, and\n",
        "2. Hit \"Kernel\"->\"Restart & Run All\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSN1KpKJtw4K"
      },
      "source": [
        "## Neural Networks: Architecture\n",
        "\n",
        "For this assignment we will explore Neural Networks; in particular, we are going to explore model complexity. We will use the same dataset from Assignment #6 to classify a mushroom as either edible ('e') or poisonous ('p'). You are free to use PyTorch, TensorFlow, scikit-learn -- to name a few resources. The goal is to explore different model complexities (architectures) before declaring a winner. Either start with a simple network and make it more complex; or start with a complex model and pare it down. Either way, your submission should clearly demonstrate your exploration.\n",
        "\n",
        "\n",
        "Your output for each model should look like the output of `cross_validate` from Assignment #6:\n",
        "\n",
        "```\n",
        "Fold: 0\tTrain Error: 15.38%\tValidation Error: 0.00%\n",
        "Fold: 1\n",
        "...\n",
        "\n",
        "Mean(Std. Dev.) over all folds:\n",
        "-------------------------------\n",
        "Train Error: 100.00%(0.00%) Test Error: 100.00%(0.00%)\n",
        "```\n",
        "\n",
        "Notice that \"Test Error\" has been replaced by \"Validation Error.\" Split your dataset into train, test, and validation sets.\n",
        "\n",
        "\n",
        "Start with a simple network. Train using the train set. Observe model's performance using the validation set.\n",
        "\n",
        "\n",
        "Increase the complexity of your network. Train using the train set. Observe model's performance using the validation set.\n",
        "\n",
        "\n",
        "Model complexity in Assignment #6 was depth limit. You can think of it here as the architecture of the network (number of layers and units per layer). Try at least three different network architectures.\n",
        "\n",
        "\n",
        "We're trying to find a model complexity that generalizes well. (Recall high bias vs high variance discussion in class.)\n",
        "\n",
        "\n",
        "Pick the network architecture that you deem best. Use the test set to report your winning model's performance. This is the ONLY time you use the test set.\n",
        "\n",
        "\n",
        "Try at least three different models; more importantly, document your process: what the results were, how the winning model was determined, what was the winning model's performance on the test data. Clearly highlight these items to receive full credit."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary packages"
      ],
      "metadata": {
        "id": "vQky6jglGbKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Documentation for Imports\n",
        "\n",
        "### `import pandas as pd`\n",
        "- **Purpose**:\n",
        "  Pandas is a powerful library for data manipulation and analysis, particularly for handling structured data like DataFrames.\n",
        "- **Usage**:\n",
        "  - Data loading from files (e.g., CSV, Excel).\n",
        "  - Data cleaning, transformation, and summarization.\n",
        "\n",
        "---\n",
        "\n",
        "### `import numpy as np`\n",
        "- **Purpose**:\n",
        "  NumPy provides support for efficient numerical computations, especially with arrays and matrices, and includes mathematical functions to operate on these structures.\n",
        "- **Usage**:\n",
        "  - Numerical operations on large datasets.\n",
        "  - Efficient handling of array-based data.\n",
        "  - Precision display settings for better readability of NumPy output.\n",
        "\n",
        "**Note**: The `np.set_printoptions(precision=3, suppress=True)` ensures NumPy arrays are displayed with 3 decimal places, and small values in scientific notation are suppressed for easier reading.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### `import tensorflow as tf`\n",
        "- **Purpose**:\n",
        "  TensorFlow is a machine learning framework used for building, training, and deploying neural networks and other ML models.\n",
        "- **Usage**:\n",
        "  - Creating and training deep learning models.\n",
        "  - Deploying models in production environments.\n",
        "\n",
        "---\n",
        "\n",
        "### `from tensorflow.keras import layers`\n",
        "- **Purpose**:\n",
        "  Provides access to TensorFlow's high-level Keras API for defining and building neural network layers.\n",
        "- **Usage**:\n",
        "  - Simplifies neural network architecture design.\n",
        "  - Supports layers like Dense, Convolutional, and Recurrent layers.\n",
        "\n",
        "---\n",
        "\n",
        "### `from sklearn.model_selection import train_test_split`\n",
        "- **Purpose**:\n",
        "  Provides a convenient method to split datasets into training and testing subsets for machine learning workflows.\n",
        "- **Usage**:\n",
        "  - Ensures that models are evaluated on unseen data to prevent overfitting.\n",
        "  - Allows for stratified splits to maintain class distributions.\n",
        "\n",
        "### ---"
      ],
      "metadata": {
        "id": "Qb2Ok22KixU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Make numpy values easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ],
      "metadata": {
        "id": "Z4Y4r4EbGfjd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "EfrcaGLU7GlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"create_folds\"></a>\n",
        "\n",
        "## create_folds\n",
        "\n",
        "The `create_folds` function partitions a dataset into a specified number of folds for use in cross-validation. It ensures that both feature (`x_data`) and target (`y_data`) datasets are split consistently, maintaining their alignment.\n",
        "\n",
        "### Parameters:\n",
        "* **x_data** `Dict[str, List[Any]]`: A dictionary where keys are feature names and values are lists containing feature data.\n",
        "* **y_data** `List[Any]`: A list containing the target values corresponding to the features in `x_data`.\n",
        "* **num_folds** `int`: The number of folds to split the dataset into.\n",
        "\n",
        "### Returns:\n",
        "* **folds_x** `List[Dict[str, List[Any]]]`: A list of dictionaries, where each dictionary contains a subset of `x_data` split into one fold.\n",
        "* **folds_y** `List[List[Any]]`: A list of lists, where each inner list contains a subset of `y_data` split into one fold.\n"
      ],
      "metadata": {
        "id": "QlMUZzVyi8NK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_folds(x_data, y_data, num_folds):\n",
        "  # Assert proper lengths of incoming data\n",
        "  assert(len(x_data.items()) > 0 and len(y_data) > 0)\n",
        "  assert(len(list(x_data.values())[0]) == len(y_data))\n",
        "\n",
        "  # Create folds of each number of elements over entire data set\n",
        "  k, m = divmod(len(y_data), num_folds)\n",
        "  folds = {'x': [], 'y': []}\n",
        "  for i in range(num_folds):\n",
        "    start = i * k + min(i, m)\n",
        "    stop = (i + 1) * k + min(i + 1, m)\n",
        "    folds['x'].append({name:values[start:stop] for name, values in x_data.items()})\n",
        "    folds['y'].append(y_data[start:stop])\n",
        "\n",
        "  return folds['x'], folds['y']"
      ],
      "metadata": {
        "id": "qadb6s-H7Jgi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"evaluate\"></a>\n",
        "## evaluate\n",
        "\n",
        "This function evaluates the accuracy of a trained model on a test dataset by comparing predicted labels to true labels.\n",
        "\n",
        "### Parameters:\n",
        "- **model** `tf.keras.Model`: The trained TensorFlow model used for making predictions.\n",
        "- **x_test_features_dict** `dict`: A dictionary of test dataset features, where keys are feature names and values are their corresponding data arrays.\n",
        "- **y_test_labels** `pandas.Series`: The ground truth labels for the test dataset.\n",
        "\n",
        "### Returns:\n",
        "- **accuracy** `float`: The proportion of correctly predicted labels to the total number of labels in the test dataset.\n",
        "\n",
        "### Notes:\n",
        "- **Prediction**: The model predicts probabilities for each test sample, which are rounded to the nearest integer to determine class labels.\n",
        "- **Evaluation**: Compares the predicted labels with the actual labels to calculate accuracy.\n",
        "- **Dependencies**:\n",
        "  - The `model.predict` method must be compatible with the input format of `x_test_features_dict`.\n",
        "  - `y_test_labels.to_list()` is used to convert the labels into a list format for comparison.\n"
      ],
      "metadata": {
        "id": "Te85Zr6alnW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, x_test_features_dict, y_test_labels):\n",
        "  # Define variables to track performance\n",
        "  correct_count = 0\n",
        "  num_elements = len(y_test_labels)\n",
        "\n",
        "  # Flatten data for easier access\n",
        "  predictions = [label for labels in model.predict(x_test_features_dict, verbose=0) for label in labels]\n",
        "  values = y_test_labels.to_list()\n",
        "\n",
        "  # Check all predictions against ground truths\n",
        "  for i in range(num_elements):\n",
        "    if values[i] == round(predictions[i]): correct_count += 1\n",
        "\n",
        "  return correct_count / num_elements"
      ],
      "metadata": {
        "id": "jt5Q8NcRlmYo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading agaricus-lepiota.data\n",
        "\n",
        "This cell performs the initial data loading and preprocessing for the mushroom classification task:\n",
        "\n",
        "1. **Load Data**:  \n",
        "   - The dataset (`agaricus-lepiota.data`) is read into a Pandas DataFrame with specified column names for clarity. The dataset contains features describing mushrooms and their labels (`poisonous` or `edible`).\n",
        "\n",
        "2. **Shuffle Data**:  \n",
        "   - The `sample(frac=1)` method shuffles the rows randomly, ensuring the data order does not bias model training. The `reset_index(drop=True)` resets the index after shuffling.\n",
        "\n",
        "3. **Preview Data**:  \n",
        "   - `mushrooms.head()` displays the first few rows of the shuffled dataset, providing a quick overview of its structure and contents.\n",
        "\n",
        "This step ensures the data is ready for further preprocessing and model training.\n"
      ],
      "metadata": {
        "id": "KogHh8ShF96a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3-RPRoiXtw4L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "54b310eb-3404-406a-82e2-3cff6ae33acb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  poisonous cap-shape cap-surface cap-color bruises odor gill-attachment  \\\n",
              "0         e         x           f         n       t    n               f   \n",
              "1         p         k           s         e       f    f               f   \n",
              "2         e         f           f         w       f    n               f   \n",
              "3         e         x           f         n       t    n               f   \n",
              "4         p         f           f         y       f    f               f   \n",
              "\n",
              "  gill-spacing gill-size gill-color  ... stalk-surface-below-ring  \\\n",
              "0            c         b          w  ...                        s   \n",
              "1            c         n          b  ...                        s   \n",
              "2            w         b          k  ...                        s   \n",
              "3            c         b          p  ...                        s   \n",
              "4            c         b          p  ...                        k   \n",
              "\n",
              "  stalk-color-above-ring stalk-color-below-ring veil-type veil-color  \\\n",
              "0                      w                      p         p          w   \n",
              "1                      w                      p         p          w   \n",
              "2                      w                      w         p          w   \n",
              "3                      w                      p         p          w   \n",
              "4                      b                      p         p          w   \n",
              "\n",
              "  ring-number ring-type spore-print-color population habitat  \n",
              "0           o         p                 n          v       d  \n",
              "1           o         e                 w          v       l  \n",
              "2           o         e                 k          s       g  \n",
              "3           o         p                 k          y       d  \n",
              "4           o         l                 h          y       d  \n",
              "\n",
              "[5 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d634eac-cf84-4dbf-9477-194c0263581b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>poisonous</th>\n",
              "      <th>cap-shape</th>\n",
              "      <th>cap-surface</th>\n",
              "      <th>cap-color</th>\n",
              "      <th>bruises</th>\n",
              "      <th>odor</th>\n",
              "      <th>gill-attachment</th>\n",
              "      <th>gill-spacing</th>\n",
              "      <th>gill-size</th>\n",
              "      <th>gill-color</th>\n",
              "      <th>...</th>\n",
              "      <th>stalk-surface-below-ring</th>\n",
              "      <th>stalk-color-above-ring</th>\n",
              "      <th>stalk-color-below-ring</th>\n",
              "      <th>veil-type</th>\n",
              "      <th>veil-color</th>\n",
              "      <th>ring-number</th>\n",
              "      <th>ring-type</th>\n",
              "      <th>spore-print-color</th>\n",
              "      <th>population</th>\n",
              "      <th>habitat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>e</td>\n",
              "      <td>x</td>\n",
              "      <td>f</td>\n",
              "      <td>n</td>\n",
              "      <td>t</td>\n",
              "      <td>n</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "      <td>w</td>\n",
              "      <td>...</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>p</td>\n",
              "      <td>n</td>\n",
              "      <td>v</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>p</td>\n",
              "      <td>k</td>\n",
              "      <td>s</td>\n",
              "      <td>e</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>n</td>\n",
              "      <td>b</td>\n",
              "      <td>...</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>e</td>\n",
              "      <td>w</td>\n",
              "      <td>v</td>\n",
              "      <td>l</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>e</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>w</td>\n",
              "      <td>f</td>\n",
              "      <td>n</td>\n",
              "      <td>f</td>\n",
              "      <td>w</td>\n",
              "      <td>b</td>\n",
              "      <td>k</td>\n",
              "      <td>...</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>e</td>\n",
              "      <td>k</td>\n",
              "      <td>s</td>\n",
              "      <td>g</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>e</td>\n",
              "      <td>x</td>\n",
              "      <td>f</td>\n",
              "      <td>n</td>\n",
              "      <td>t</td>\n",
              "      <td>n</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "      <td>p</td>\n",
              "      <td>...</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>p</td>\n",
              "      <td>k</td>\n",
              "      <td>y</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>p</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>y</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "      <td>p</td>\n",
              "      <td>...</td>\n",
              "      <td>k</td>\n",
              "      <td>b</td>\n",
              "      <td>p</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>l</td>\n",
              "      <td>h</td>\n",
              "      <td>y</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 23 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d634eac-cf84-4dbf-9477-194c0263581b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9d634eac-cf84-4dbf-9477-194c0263581b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9d634eac-cf84-4dbf-9477-194c0263581b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-69feb29f-4906-49fd-8b05-5957e2ae90f8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-69feb29f-4906-49fd-8b05-5957e2ae90f8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-69feb29f-4906-49fd-8b05-5957e2ae90f8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "mushrooms"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Implementation and exploration.\n",
        "mushrooms = pd.read_csv(\"agaricus-lepiota.data\", names=[\"poisonous\", \"cap-shape\", \"cap-surface\", \"cap-color\", \"bruises\", \"odor\",\n",
        "           \"gill-attachment\", \"gill-spacing\", \"gill-size\", \"gill-color\", \"stalk-shape\", \"stalk-root\", \"stalk-surface-above-ring\",\n",
        "           \"stalk-surface-below-ring\", \"stalk-color-above-ring\", \"stalk-color-below-ring\", \"veil-type\", \"veil-color\",\n",
        "           \"ring-number\", \"ring-type\", \"spore-print-color\", \"population\", \"habitat\"])\n",
        "\n",
        "# Shuffle data\n",
        "mushrooms = mushrooms.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Preview data\n",
        "mushrooms.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format agaricus-lepiota.data for preprocessing\n",
        "\n",
        "This cell prepares the dataset for training and testing by splitting it into features and labels, and further into training and test sets:\n",
        "\n",
        "1. **Separate Features and Labels**:  \n",
        "   - A copy of the `mushrooms` dataset is created as `mushroom_features` to ensure the original dataset remains unmodified.\n",
        "   - The target variable, `poisonous`, is extracted into `mushroom_labels` and mapped to binary values: `'e'` (edible) to `0` and `'p'` (poisonous) to `1`.\n",
        "\n",
        "2. **Convert Features to Dictionary**:  \n",
        "   - `mushroom_features_dict` converts the features into a dictionary format, where each column name is a key, and its values are stored as NumPy arrays. This format aligns with TensorFlow's input requirements.\n",
        "\n",
        "3. **Train-Test Split**:  \n",
        "   - The data is split into training (80%) and testing (20%) subsets using `train_test_split`. A `random_state` ensures reproducibility of the split.\n",
        "\n",
        "4. **Prepare Dictionary Formats**:  \n",
        "   - Both `x_train_features` and `x_test_features` are converted into dictionary formats (`x_train_features_dict` and `x_test_features_dict`), similar to the complete dataset.\n",
        "\n",
        "This step ensures the data is appropriately formatted and split for model training, validation, and final evaluation.\n"
      ],
      "metadata": {
        "id": "Pno2ClD_JFHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Pandas DataFrame and Series from data\n",
        "mushroom_features = mushrooms.copy()\n",
        "mushroom_labels = mushroom_features.pop('poisonous').map({'e': 0, 'p': 1})\n",
        "mushroom_features_dict = {name: np.array(value)\n",
        "                          for name, value in mushroom_features.items()}\n",
        "\n",
        "# Split data into training and testing sets\n",
        "x_train_features, x_test_features, y_train_labels, y_test_labels = train_test_split(\n",
        "    mushroom_features, mushroom_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Format data for TensorFlow\n",
        "x_train_features_dict = {name: np.array(value) for name, value in x_train_features.items()}\n",
        "x_test_features_dict = {name: np.array(value) for name, value in x_test_features.items()}"
      ],
      "metadata": {
        "id": "sCF-3aOW-1W9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"evaluate_mushroom_model\"></a>\n",
        "## evaluate_mushroom_model\n",
        "\n",
        "This function builds, compiles, and trains a TensorFlow model for classifying mushrooms based on the provided training and validation datasets. The model uses one-hot encoding for feature preprocessing and supports customizable complexity via user-defined dense layer configurations.\n",
        "\n",
        "### Parameters:\n",
        "- **x_train** `dict`: A dictionary containing the training dataset features, where keys are feature names and values are their corresponding data arrays (strings).\n",
        "- **y_train** `array-like`: The training dataset labels.\n",
        "- **epochs** `int`: The number of epochs for training the model.\n",
        "- **x_val** `dict`: A dictionary containing the validation dataset features, formatted similarly to `x_train`.\n",
        "- **y_val** `array-like`: The validation dataset labels.\n",
        "- **model_complexity** `list[int]`: A list defining the number of units in each dense layer, determining the model's architecture.\n",
        "- **activation** `str`: The activation function used in the neural network layers (e.g., `\"relu\"`, `\"sigmoid\"`).\n",
        "- **optimizer** `str or object`: The optimizer used for training the model (e.g., `\"adam\"`, `\"sgd\"`).\n",
        "- **loss** `str or object`: The loss function used for training the model (e.g., `\"binary_crossentropy\"`, `\"mean_squared_error\"`).\n",
        "\n",
        "### Returns:\n",
        "- **model** `tf.keras.Model`: The trained TensorFlow model.\n",
        "- **history** `tf.keras.callbacks.History`: The training history object, which includes metrics and loss values for each epoch.\n",
        "\n",
        "### Notes:\n",
        "- **Feature Preprocessing**: The function uses a `StringLookup` layer to convert string-based features into one-hot encoded vectors.\n",
        "- **Model Complexity**: The architecture is determined dynamically based on the `model_complexity` parameter, allowing for flexible network designs.\n",
        "- **Loss Function**: Uses binary cross-entropy for binary classification tasks.\n",
        "- **Metrics**: Tracks accuracy during training and validation.\n",
        "\n",
        "### Dependencies:\n",
        "- Requires TensorFlow (`tf`) and Keras layers (`layers`) for building and training the model.\n"
      ],
      "metadata": {
        "id": "NLKp-pMHlgmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_mushroom_model(x_train, y_train, epochs, x_val, y_val, model_complexity, activation, optimizer, loss):\n",
        "    # Build inputs\n",
        "    inputs = {name: tf.keras.Input(shape=(1,), name=name, dtype=tf.string) for name in x_train.keys()}\n",
        "\n",
        "    # Preprocess inputs\n",
        "    encoded_features = []\n",
        "    for name in inputs.keys():\n",
        "        # String lookup layer\n",
        "        lookup = layers.StringLookup(output_mode='one_hot')\n",
        "        lookup.adapt(x_train[name])\n",
        "        encoded_feature = lookup(inputs[name])\n",
        "        encoded_features.append(encoded_feature)\n",
        "\n",
        "    # Concatenate all features\n",
        "    all_features = layers.concatenate(encoded_features)\n",
        "\n",
        "    # Build the model with the specified complexity\n",
        "    x = all_features\n",
        "    count = 0\n",
        "    for units in model_complexity:\n",
        "        x = layers.Dense(units, activation=activation, name=f'dense_layer_{count}')(x)\n",
        "        count += 1\n",
        "    output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=loss,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Fit the model\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        validation_data=(x_val, y_val),\n",
        "                        epochs=epochs,\n",
        "                        verbose=0)\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "OOHFynWVNeX3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"run_experiment\"></a>\n",
        "## run_experiment\n",
        "\n",
        "Runs a cross-validation experiment to evaluate a neural network model with the specified configuration. It trains and validates the model across multiple folds, calculates training and validation errors, and reports the performance.\n",
        "\n",
        "### Parameters:\n",
        "* **complexity** `list[int]`: A list defining the architecture of the neural network (e.g., layer sizes).\n",
        "* **x_folds** `list[dict[str, np.ndarray]]`: A list where each element is a dictionary of feature arrays for a specific fold.\n",
        "* **y_folds** `list[np.ndarray]`: A list where each element is an array of labels for a specific fold.\n",
        "* **activation** `str`: The activation function used in the neural network layers (e.g., `\"relu\"`, `\"sigmoid\"`).\n",
        "* **optimizer** `str or object`: The optimizer used for training the model (e.g., `\"adam\"`, `\"sgd\"`).\n",
        "* **loss** `str or object`: The loss function used for training the model (e.g., `\"binary_crossentropy\"`, `\"mean_squared_error\"`).\n",
        "* **num_epochs** `int`: Number of epochs to train the model.\n",
        "\n",
        "### Returns:\n",
        "* **model** `object`: The trained neural network model.\n",
        "* **avg_val_error** `float`: Average validation error across all folds.\n"
      ],
      "metadata": {
        "id": "hkU9JEZ41x_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(complexity, x_folds, y_folds, activation, optimizer, loss, num_epochs):\n",
        "\n",
        "  # Create data structures to log model performance\n",
        "  fold_train_errors = []\n",
        "  fold_val_errors = []\n",
        "  print(f\"\\nEvaluating model with layers: {complexity}\")\n",
        "\n",
        "  for i in range(num_folds):\n",
        "      # Prepare validation data\n",
        "      x_val_fold, y_val_fold = x_folds[i], y_folds[i]\n",
        "\n",
        "      # Prepare training data by combining all other folds\n",
        "      x_train_folds = {\n",
        "          key: np.concatenate([x_folds[j][key] for j in range(num_folds) if j != i])\n",
        "          for key in x_train_features.columns\n",
        "      }\n",
        "      y_train_folds = np.concatenate([y_folds[j] for j in range(num_folds) if j != i])\n",
        "\n",
        "      # Build and evaluate the model\n",
        "      model, history = evaluate_mushroom_model(\n",
        "          x_train_folds, y_train_folds, num_epochs, x_val_fold, y_val_fold,\n",
        "          complexity, activation, optimizer, loss\n",
        "      )\n",
        "\n",
        "      # Calculate errors\n",
        "      fold_train_errors.append(100 * (1 - np.mean(history.history['accuracy'])))\n",
        "      fold_val_errors.append(100 * (1 - np.mean(history.history['val_accuracy'])))\n",
        "\n",
        "  # Display fold results for this model complexity\n",
        "  for i, (train_err, val_err) in enumerate(zip(fold_train_errors, fold_val_errors)):\n",
        "      print(f\"Fold: {i}    Train Error: {train_err:.2f}%    Validation Error: {val_err:.2f}%\")\n",
        "\n",
        "  # Report model performance\n",
        "  mean_train_error = np.mean(fold_train_errors)\n",
        "  std_train_error = np.std(fold_train_errors)\n",
        "  mean_val_error = np.mean(fold_val_errors)\n",
        "  std_val_error = np.std(fold_val_errors)\n",
        "\n",
        "  print(\"\\nMean(Std. Dev.) over all folds for this model:\")\n",
        "  print(\"-------------------------------\")\n",
        "  print(f\"Train Error: {mean_train_error:.2f}%({std_train_error:.2f}%) Validation Error: {mean_val_error:.2f}%({std_val_error:.2f}%)\")\n",
        "\n",
        "  return model, np.mean(fold_val_errors)"
      ],
      "metadata": {
        "id": "F-xJOgKS1vLV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup experiment\n",
        "\n",
        "This cell is a setup for the cross-validation experiments and exploration of different neural network architectures:\n",
        "\n",
        "1. **Number of Folds**:  \n",
        "   - `num_folds = 4`: The dataset is divided into 4 folds for cross-validation. This ensures that every part of the data is used for both training and validation, reducing the risk of overfitting and providing a more robust evaluation of the model's performance.\n",
        "\n",
        "2. **Number of Epochs**:  \n",
        "   - `num_epochs = 1`: Each model is trained for a single epoch to quickly explore the effects of different architectures. While this limits convergence, it allows for faster experimentation.\n",
        "\n",
        "3. **Create Folds**:  \n",
        "   - `create_folds(x_train_features_dict, y_train_labels.values, num_folds)`: The training data is partitioned into folds for cross-validation. Each fold contains a subset of features (`x`) and corresponding labels (`y`).\n",
        "\n",
        "4. **Model Complexities**:  \n",
        "   - The `model_complexities` list defines three different architectures to explore:\n",
        "     - `[16]`: A simple model with a single layer containing 16 units.\n",
        "     - `[32, 16]`: A moderately complex model with two layers containing 32 and 16 units, respectively.\n",
        "     - `[64, 32, 16]`: A high-complexity model with three layers containing 64, 32, and 16 units, respectively.\n",
        "   - These architectures are chosen to systematically evaluate the impact of increasing network complexity on model performance.\n",
        "\n",
        "This setup facilitates a systematic exploration of model performance across different architectures, allowing the best-performing configuration to be identified through cross-validation."
      ],
      "metadata": {
        "id": "Svm-8InuJi2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_folds = 4\n",
        "num_epochs = 1\n",
        "\n",
        "# Create folds from the training data\n",
        "x, y = create_folds(x_train_features_dict, y_train_labels.values, num_folds)\n",
        "\n",
        "# Define different model complexities to try\n",
        "model_complexities = [\n",
        "    [16],                # Simple model\n",
        "    [32, 16],            # Moderate complexity\n",
        "    [64, 32, 16],        # High complexity\n",
        "]"
      ],
      "metadata": {
        "id": "TvKXPo9NJg5u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conduct experiment for all three complexities\n",
        "\n",
        "### **Experiment 1: Baseline Network ([16])**\n",
        "- **Motivation**:  \n",
        "  Start with a simple neural network architecture to establish a baseline. A single-layer model with 16 units is chosen to minimize complexity while still providing meaningful classification performance. Simple architectures are less prone to overfitting and provide insights into the dataset's separability.\n",
        "\n",
        "- **Observations**:  \n",
        "  - **Validation Performance**: Achieved a mean validation error of **2.97% (±0.90%)**.  \n",
        "  - **Training Error**: Recorded a mean training error of **12.61% (±1.70%)**.\n",
        "\n",
        "- **Effect of Simplicity**:  \n",
        "  - Low validation error indicates reasonable generalization to unseen data.  \n",
        "  - Higher training error suggests underfitting, which is expected for a minimal architecture.\n",
        "\n",
        "---\n",
        "\n",
        "### **Experiment 2: Moderate Complexity ([32, 16])**\n",
        "- **Motivation**:  \n",
        "  Increase the model's capacity to capture more complex patterns by introducing an additional layer with 32 units. This moderate increase aims to balance underfitting and overfitting.\n",
        "\n",
        "- **Observations**:  \n",
        "  - **Validation Performance**: Mean validation error improved to **0.97% (±0.13%)**.  \n",
        "  - **Training Error**: Reduced to **7.29% (±1.88%)**.\n",
        "\n",
        "- **Impact of Increased Complexity**:  \n",
        "  - Lower validation error compared to the baseline indicates improved generalization.  \n",
        "  - Reduced training error confirms the model's ability to better capture data patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### **Experiment 3: High Complexity ([64, 32, 16])**\n",
        "- **Motivation**:  \n",
        "  Further increase the model's capacity by adding a third layer with 64 units to test whether additional complexity improves performance or leads to overfitting.\n",
        "\n",
        "- **Observations**:  \n",
        "  - **Validation Performance**: Achieved the best mean validation error of **0.08% (±0.03%)**.  \n",
        "  - **Training Error**: Further decreased to **5.58% (±1.06%)**.\n",
        "\n",
        "- **Insights**:  \n",
        "  - Improved validation performance suggests the model can generalize very well without overfitting.  \n",
        "  - Reduced training error highlights the model's enhanced learning capacity."
      ],
      "metadata": {
        "id": "9z3qNneVJl2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_validation_error = float('inf')\n",
        "best_model = None\n",
        "best_complexity = None\n",
        "\n",
        "activation = 'relu'\n",
        "optimizer = 'adam'\n",
        "loss = 'binary_crossentropy'\n",
        "\n",
        "# Conduct experiment on all model complexities\n",
        "for complexity in model_complexities:\n",
        "    model, avg_val_error = run_experiment(complexity, x, y, activation, optimizer, loss, num_epochs)\n",
        "\n",
        "    # Update best model if current one is better\n",
        "    if avg_val_error < best_validation_error:\n",
        "        best_validation_error = avg_val_error\n",
        "        best_model = model\n",
        "        best_complexity = complexity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvBBVN8BvLvj",
        "outputId": "a3a6b830-7270-4222-af6c-b21d41f4d185"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model with layers: [16]\n",
            "Fold: 0    Train Error: 14.71%    Validation Error: 3.57%\n",
            "Fold: 1    Train Error: 12.00%    Validation Error: 1.78%\n",
            "Fold: 2    Train Error: 13.54%    Validation Error: 4.06%\n",
            "Fold: 3    Train Error: 10.17%    Validation Error: 2.46%\n",
            "\n",
            "Mean(Std. Dev.) over all folds for this model:\n",
            "-------------------------------\n",
            "Train Error: 12.61%(1.70%) Validation Error: 2.97%(0.90%)\n",
            "\n",
            "Evaluating model with layers: [32, 16]\n",
            "Fold: 0    Train Error: 4.23%    Validation Error: 0.80%\n",
            "Fold: 1    Train Error: 7.65%    Validation Error: 0.92%\n",
            "Fold: 2    Train Error: 9.36%    Validation Error: 0.98%\n",
            "Fold: 3    Train Error: 7.94%    Validation Error: 1.17%\n",
            "\n",
            "Mean(Std. Dev.) over all folds for this model:\n",
            "-------------------------------\n",
            "Train Error: 7.29%(1.88%) Validation Error: 0.97%(0.13%)\n",
            "\n",
            "Evaluating model with layers: [64, 32, 16]\n",
            "Fold: 0    Train Error: 4.47%    Validation Error: 0.06%\n",
            "Fold: 1    Train Error: 4.60%    Validation Error: 0.06%\n",
            "Fold: 2    Train Error: 6.81%    Validation Error: 0.06%\n",
            "Fold: 3    Train Error: 6.44%    Validation Error: 0.12%\n",
            "\n",
            "Mean(Std. Dev.) over all folds for this model:\n",
            "-------------------------------\n",
            "Train Error: 5.58%(1.06%) Validation Error: 0.08%(0.03%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate best model's performance on test data\n",
        "\n",
        "This cell evaluates the best-performing model, determined during the cross-validation phase, on the independent test dataset:\n",
        "\n",
        "1. **Identify the Best Model**:  \n",
        "   - The `best_complexity` variable stores the architecture of the model that achieved the lowest validation error during cross-validation.\n",
        "   - `print(f\"Best model complexity: {best_complexity}\")` displays the architecture of the chosen model for reference. In this case, the best model has three layers with 64, 32, and 16 units.\n",
        "\n",
        "2. **Test Set Evaluation**:  \n",
        "   - The `evaluate` function calculates the accuracy of the `best_model` using the preprocessed test dataset (`x_test_features_dict` and `y_test_labels`).\n",
        "   - The test accuracy is used to compute the **test error** as `100 * (1 - test_accuracy)`.\n",
        "\n",
        "3. **Report Test Error**:  \n",
        "   - The `print` statement outputs the test error, which provides an estimate of the model's ability to generalize to completely unseen data.\n",
        "   - In this instance, the test error is **0.06%**, indicating exceptional generalization performance.\n",
        "\n",
        "This step is crucial because the test set is used only once, after the model has been finalized, to avoid overfitting and ensure an unbiased evaluation of the model's real-world performance.\n"
      ],
      "metadata": {
        "id": "sc39ghkXKAt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After trying all models, evaluate the best one on the test set\n",
        "print(f\"Best model complexity: {best_complexity}\")\n",
        "\n",
        "# Evaluate the model using the preprocessed test data\n",
        "test_accuracy = evaluate(best_model, x_test_features_dict, y_test_labels)\n",
        "test_error = 100 * (1 - test_accuracy)\n",
        "print(f\"Test Error of the best model: {test_error:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-YDBjBFJ_8h",
        "outputId": "d29332cd-5258-47cc-b616-0e18bf92bfe3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model complexity: [64, 32, 16]\n",
            "Test Error of the best model: 0.06%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeaSsuWCt2dI"
      },
      "source": [
        "## Experiment: Activation Function and Optimizer\n",
        "Modify the 1) Activation function 2) Optimizer of any chosen model. Try at least one model for each modified component.\n",
        "\n",
        "Explain the motivation behind the modifications you made.\n",
        "\n",
        "Explore the effects on the performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Experiment 4: Modify Activation function from 'relu' to 'tanh' (optimizer 'adam')**\n",
        "- **Motivation**:  \n",
        "  Replace ReLU with **tanh** to explore its ability to capture non-linear patterns symmetrically, given the dataset features.\n",
        "\n",
        "- **Observations**:  \n",
        "  - **Validation Performance**: Slightly worsened to **0.25% (±0.20%)**.  \n",
        "  - **Training Error**: Slightly improved to **4.13% (±0.48%)**.\n",
        "\n",
        "- **Analysis**:  \n",
        "  - The shift to tanh led to a minor drop in generalization performance, possibly due to vanishing gradients during training.  \n",
        "  - Training error decreased, but the higher validation error suggests reduced efficacy compared to ReLU.\n"
      ],
      "metadata": {
        "id": "bp3UzI_u7YIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "activation = 'tanh'\n",
        "optimizer = 'adam'\n",
        "loss = 'binary_crossentropy'\n",
        "complexity = best_complexity\n",
        "\n",
        "model, avg_val_error = run_experiment(complexity, x, y, activation, optimizer, loss, num_epochs)\n",
        "\n",
        "# Evaluate the new model using the preprocessed test data\n",
        "print(f\"\\nModel complexity: {complexity}\")\n",
        "test_accuracy = evaluate(model, x_test_features_dict, y_test_labels)\n",
        "test_error = 100 * (1 - test_accuracy)\n",
        "print(f\"Test Error of the model: {test_error:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZX0WcKwK6rAe",
        "outputId": "c5c25837-0f5a-4380-dd69-39debe242412"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model with layers: [64, 32, 16]\n",
            "Fold: 0    Train Error: 4.31%    Validation Error: 0.00%\n",
            "Fold: 1    Train Error: 3.39%    Validation Error: 0.18%\n",
            "Fold: 2    Train Error: 4.72%    Validation Error: 0.25%\n",
            "Fold: 3    Train Error: 4.12%    Validation Error: 0.55%\n",
            "\n",
            "Mean(Std. Dev.) over all folds for this model:\n",
            "-------------------------------\n",
            "Train Error: 4.13%(0.48%) Validation Error: 0.25%(0.20%)\n",
            "\n",
            "Model complexity: [64, 32, 16]\n",
            "Test Error of the model: 0.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Experiment 5: Modify Optimizer from 'adam' to 'sgd' gradient descent w/ momentum (activation function 'relu')**\n",
        "- **Motivation**:  \n",
        "  Replace Adam with **SGD** to evaluate the impact of a simpler optimization algorithm that relies on manual momentum tuning.\n",
        "\n",
        "- **Observations**:  \n",
        "  - **Validation Performance**: Validation error significantly rose to **11.51% (±1.48%)**.  \n",
        "  - **Training Error**: Also significantly increased to **23.81% (±6.40%)**.\n",
        "\n",
        "- **Insights**:  \n",
        "  - The model struggled with convergence using SGD, resulting in considerably worse performance.  \n",
        "  - Highlights the importance of adaptive optimizers like Adam for this problem.\n"
      ],
      "metadata": {
        "id": "6MB6uORe7c6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "activation = 'relu'\n",
        "optimizer = 'sgd'\n",
        "loss = 'binary_crossentropy'\n",
        "complexity = best_complexity\n",
        "\n",
        "model, avg_val_error = run_experiment(complexity, x, y, activation, optimizer, loss, num_epochs)\n",
        "\n",
        "# Evaluate the new model using the preprocessed test data\n",
        "print(f\"\\nModel complexity: {complexity}\")\n",
        "test_accuracy = evaluate(model, x_test_features_dict, y_test_labels)\n",
        "test_error = 100 * (1 - test_accuracy)\n",
        "print(f\"Test Error of the model: {test_error:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR29d7rX7_vT",
        "outputId": "71676492-ee10-4fe4-e29d-f415343041c5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model with layers: [64, 32, 16]\n",
            "Fold: 0    Train Error: 18.10%    Validation Error: 12.68%\n",
            "Fold: 1    Train Error: 34.22%    Validation Error: 13.17%\n",
            "Fold: 2    Train Error: 19.02%    Validation Error: 10.65%\n",
            "Fold: 3    Train Error: 23.92%    Validation Error: 9.54%\n",
            "\n",
            "Mean(Std. Dev.) over all folds for this model:\n",
            "-------------------------------\n",
            "Train Error: 23.81%(6.40%) Validation Error: 11.51%(1.48%)\n",
            "\n",
            "Model complexity: [64, 32, 16]\n",
            "Test Error of the model: 8.31%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJDsC809yqTw"
      },
      "source": [
        "## OPTIONAL. BONUS. Experiment: Loss Function\n",
        "\n",
        "Modify the loss function of any chosen model.\n",
        "\n",
        "Explain the motivation behind the modifications you made.\n",
        "\n",
        "Explore the effects on the performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Experiment 6: Modify Loss from 'binary_crossentropy' to 'mean_squared_error' (activation function 'relu' and optimizer 'sgd')**\n",
        "- **Motivation**:  \n",
        "  Replace binary cross-entropy with **mean squared error (MSE)** to analyze the effects of a regression-based loss for classification tasks.\n",
        "\n",
        "- **Observations**:  \n",
        "  - **Validation Performance**: Remained excellent at **0.17% (±0.13%)**.  \n",
        "  - **Training Error**: Slightly better at **4.03% (±0.27%)**.\n",
        "\n",
        "- **Analysis**:  \n",
        "  - MSE performed similarly to binary cross-entropy, suggesting it was able to handle the binary classification task effectively.  \n",
        "  - However, binary cross-entropy is theoretically more suitable for probabilistic outputs.\n"
      ],
      "metadata": {
        "id": "64aZdW5g8HXi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9zavtQjCy7Ud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44aaf2ee-f22f-43c6-ea59-fe4e205eb694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model with layers: [64, 32, 16]\n",
            "Fold: 0    Train Error: 4.02%    Validation Error: 0.00%\n",
            "Fold: 1    Train Error: 3.78%    Validation Error: 0.37%\n",
            "Fold: 2    Train Error: 3.86%    Validation Error: 0.18%\n",
            "Fold: 3    Train Error: 4.47%    Validation Error: 0.12%\n",
            "\n",
            "Mean(Std. Dev.) over all folds for this model:\n",
            "-------------------------------\n",
            "Train Error: 4.03%(0.27%) Validation Error: 0.17%(0.13%)\n",
            "\n",
            "Model complexity: [64, 32, 16]\n",
            "Test Error of the model: 0.06%\n"
          ]
        }
      ],
      "source": [
        "activation = 'relu'\n",
        "optimizer = 'adam'\n",
        "loss = 'mean_squared_error'\n",
        "complexity = best_complexity\n",
        "\n",
        "model, avg_val_error = run_experiment(complexity, x, y, activation, optimizer, loss, num_epochs)\n",
        "\n",
        "# Evaluate the new model using the preprocessed test data\n",
        "print(f\"\\nModel complexity: {complexity}\")\n",
        "test_accuracy = evaluate(model, x_test_features_dict, y_test_labels)\n",
        "test_error = 100 * (1 - test_accuracy)\n",
        "print(f\"Test Error of the model: {test_error:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "- **Best Model**: The architecture with three layers ([64, 32, 16]) and ReLU activation, Adam optimizer, and binary cross-entropy loss performed the best.\n",
        "- **Test Performance**: This model achieved a **test error of 0.06%**, demonstrating exceptional generalization."
      ],
      "metadata": {
        "id": "tnscIwhjE4Z8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMCU5PBHz8DF"
      },
      "source": [
        "No other directions for this assignment, other than what's here and in the \"General Directions\" section. You have a lot of freedom with this assignment. Don't get carried away. It is expected the results may vary, being better or worse, due to the limitations of the dataset. Graders are not going to run your notebooks. The notebook will be read as a report on how different models were explored. Since you'll be using libraries, the emphasis will be on your ability to communicate your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VfoAYAQtw4M"
      },
      "source": [
        "## Before You Submit...\n",
        "\n",
        "1. Re-read the general instructions provided above, and\n",
        "2. Hit \"Kernel\"->\"Restart & Run All\"."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "81px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}